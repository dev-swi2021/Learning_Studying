{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"goodbooks-10k_Contents_Based_Model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMkmVPHC42LbwIgow6V2GtY"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kjMtzN-IhT8-"},"source":["# Package Import\r\n","- Link : https://www.kaggle.com/zygmunt/goodbooks-10k\r\n","- Reference : https://www.kaggle.com/chocozzz/01-goodbooks-10k-data-exploratory-analysis"]},{"cell_type":"code","metadata":{"id":"H8djrquJhQmE"},"source":["import pandas as pd\r\n","import numpy as np\r\n","import plotnine\r\n","from plotnine import *\r\n","import os, sys, gc\r\n","from tqdm.notebook import tqdm\r\n","import warnings\r\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2kNy-ga1FqG"},"source":["path = '../input/t-academy-recommendation2/books/'\r\n","print(os.listdir(path))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2DPPgQ8ahwsb"},"source":["books = pd.read_csv(path + \"books.csv\")\r\n","book_tags = pd.read_csv(path + \"book_tags.csv\")\r\n","ratings = pd.read_csv(path + \"ratings.csv\")\r\n","tags = pd.read_csv(path + \"tags.csv\")\r\n","to_read = pd.read_csv(path + \"to_read.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cEGP1cu69yh2"},"source":["train['book_id'] = train['book_id'].astype(str)\r\n","test['book_id'] = test['book_id'].astype(str)\r\n","books['book_id'] = books['book_id'].astype(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y8F9Yq9mZrZI"},"source":["popular_rec_model = books.sort_values(by='books_count', ascending=False)['book_id'].values[0:500]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgWOuUZpZxs5"},"source":["sol = test.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\r\n","gt = {}\r\n","for user in tqdm(sol['user_id'].unique()):\r\n","    gt[user] = list(sol[sol['user_id'] == user]['unique'].values[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqwDZOQDabK9"},"source":["rec_df = pd.DataFrame()\r\n","rec_df['user_id'] = train['user_id'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9GVfHIexaiKW"},"source":["## TF-IDF를 이용한 Contents Based Model"]},{"cell_type":"code","metadata":{"id":"Qz8RXSPtalo9"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\r\n","tfidf = TfidfVectorizer(stop_words='english')\r\n","tfidf_matrix = tfidf.fit_transform(books['title'])\r\n","print(tfidf_matrix.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYrcqW5zbCn7"},"source":["from sklearn.metrics.pairwise import cosine_similarity\r\n","cosine_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\r\n","cosine_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iyXWkZxSbOds"},"source":["# book title와 id를 매핑할 dictionary를 생성해줍니다.\r\n","book2id = {}\r\n","\r\n","for i, c in enumerate(books['title']): book2id[i] = c\r\n","\r\n","# id와 book title를 매핑할 dictionary를 생성해줍니다.\r\n","id2book = {}\r\n","for i, c in book2id.items(): id2book[c] = i\r\n","\r\n","# book_id와 totle를 매핑할 dictionary를 생성해줍니다.\r\n","bookid2book = {}\r\n","for i, j in zip(books['title'].values, books['book_id'].values):\r\n","    bookid2book[i] = j"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jB2rUFMibyE8"},"source":["books['title'].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R8C0ELXabzdp"},"source":["idx = id2book['Twilight (Twilight, #1)']\r\n","sim_scores = [(book2id[i],c) for i,c in enumerate(cosine_matrix[idx]) if i != idx]\r\n","sim_scores[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1hKF0XgEcAOp"},"source":["1. 학습셋에서 제목이 있는 경우에 대해서만 진행  \r\n","2. 각 유저별로 읽은 책의 목록을 수집  \r\n","3. 읽은 책과 유사한 책 추출  \r\n","4. 모든 책에 대해서 유사도를 더한 값을 계산  \r\n","5. 3에서 유사도가 가장 높은 순서대로 추출"]},{"cell_type":"code","metadata":{"id":"n2mqVq-wb_nL"},"source":["train = pd.merge(train, books[['book_id', 'title']], how='left', on='book_id')\r\n","train.heaD()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngSfkNwxcULp"},"source":["# 0. 학습셋에서 제목이 있는 경우에 대해서만 진행\r\n"," tf_train = train[train['title'].notnull()].reset_index(drop=True)\r\n"," tf_train['idx2title'] = tf_train['title'].apply(lambda x:id2book[x])\r\n"," tf_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wUE40-PJd-k1"},"source":["idx2title2book = {}\r\n","for i,j in zip(tf_train['idx2title'].values, tf_train['book_id'].values):\r\n","    idx2title2book[i] = j"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aBLaaT4YeKgy"},"source":["# 1. 각 유저별로 읽은 책의 목록을 수집\r\n","user = 7\r\n","read_list = tf_train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\r\n","seen = read_list[read_list['user_id'] == user]['unique'].values[0]\r\n","seen"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdDSZm9UeyXY"},"source":["# 2. 읽은 책과 유사한 책 추출\r\n","### 343번째 책과 다른 책들간의 유사도\r\n","cosine_matrix[343]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Du5SfvaIe4p_"},"source":["# 2.읽은 책과 유사한 책 추출\r\n","total_cosine_sim = np.zeros(len(book2id))\r\n","for book_ in seen:\r\n","    # 3. 모든 책에 대해서 유사도를 더한 값을 계산\r\n","    # 343번째 책과 248의 유사도가 모두 결합된 유사도\r\n","    total_cosine_sim += cosine_matrix[book_]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSMHHhIPgrvM"},"source":["# 4. 3에서 유사도가 가장 높은 순서대로 추출\r\n","sim_scores = [(i,c) for i,c in enumerate(total_cosine_sim) if i not in seen]    # 자기 자신을 제외한 영화들의 유사도 및 인덱스를 추출\r\n","sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\r\n","sim_scores[0:5]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgBFpqwxiBMK"},"source":["book2id[4809]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jg8Ise47iC1e"},"source":["bookid2book[book2id[4809]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Plzypk-miFOf"},"source":["tf_train['user_id'].unique()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sE1nGqimjq0b"},"source":["tf_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZEVMcHDjr2x"},"source":["## 전체 영화에 대해서 진행\r\n","total_rec_list = {}\r\n","\r\n","read_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\r\n","read_list2 = train.groupby(['user_id'])['idx2title'].agg({'unique'}).reset_index()\r\n","\r\n","for user in tqdm(train['user_id'].unique()):\r\n","    rec_list = []\r\n","\r\n","    # 만약 TF-IDF 소속의 추천대상이라면 Contents 기반의 추천\r\n","    if user in tf_train['user_id'].unique():\r\n","        # 1. 각 유저별로 읽은 책의 목록을 수집\r\n","        seen = read_list2[read_list2['user_id'] == user]['unique'].values[0]\r\n","\r\n","        # 2. 읽은 책과 유사한 책 추출\r\n","        total_cosine_sim = np.zeros(len(book2id))\r\n","        for book_ in seen:\r\n","            # 3. 모든 책에 대해서 유사도를 더한 값을 계산\r\n","            # 343번째 책과 248의 유사도가 모두 결합된 유사도\r\n","            total_cosine_sim += cosine_matrix[book_]\r\n","        \r\n","        # 4. 3에서 유사도가 가장 높은 순서대로 추출\r\n","        sim_scores = [(bookid2book[book2id[i]], c) for i, c in enumerate(total_cosine_sim) if i not in seen]    # 자기 자신을 제외한 영화들의 유사도 및 인덱스를 추출\r\n","        recs = sorted(sim_scores, key=lambda x:x[1], reverse=True)[0:300]   # 유사도가 높은 순서대로 정렬\r\n","\r\n","        for rec in recs:\r\n","            if rec not in seen:\r\n","                rec_list.append(rec)\r\n","    # 그렇치 않으면 인기도 기반의 추천\r\n","    else:\r\n","        seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\r\n","        for rec in popular_rec_model[0:400]:\r\n","            if rec not in seen:\r\n","                rec_list.append(rec)\r\n","    \r\n","    total_rec_list[user] = rec_list[0:200]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g5wy9X0HmR7j"},"source":["import six\r\n","import math\r\n","\r\n","# https://github.com/kakao-arena/brunch-article-recommendation/blob/master/evaluate.py\r\n","\r\n","class evaluate():\r\n","    def __init__(self, rec, gt, topn=100):\r\n","        self.recs = recs\r\n","        self.gt = gt\r\n","        self.topn = topn\r\n","    \r\n","    def _ndcg(self):\r\n","        Q, S = 0.0, 0.0\r\n","        for u, seen in six.iteritems(self.gt):\r\n","            seen = list(set(seen))\r\n","            rec = self.recs.get(u, [])\r\n","            if not rec or len(seen) == 0:\r\n","                continue\r\n","            \r\n","            dcg = 0.0\r\n","            idcg = sum([1.0 / math.log(i+2, 2) for i in range(min(len(seen), len(rec)))])\r\n","            for i, r in enumerate(rec):\r\n","                if r not in seen:\r\n","                    continue\r\n","                rank = i + 1\r\n","                dcg += 1.0 / math.log(rank + 1, 2)\r\n","            ndcg = dcg / idcg\r\n","            S += ndcg\r\n","            Q += 1\r\n","        return S / Q\r\n","    \r\n","    def _map(self):\r\n","        n, ap = 0.0, 0.0\r\n","        for u, seen in six.iteritems(self.gt):\r\n","            seen = list(set(seen))\r\n","            rec = self.recs.get(u, [])\r\n","            if not rec or len(seen) == 0:\r\n","                continue\r\n","            \r\n","            _ap, correct = 0.0, 0.0\r\n","            for i, r in enumerate(rec):\r\n","                if r in seen:\r\n","                    correct += 1\r\n","                    _ap += (correct / (i + 1.0))\r\n","            _ap /= min(len(seen), len(rec))\r\n","            ap += _ap\r\n","            n += 1.0\r\n","        return ap / n\r\n","    \r\n","    def _entropy_diversity(self):\r\n","        sz = float(len(self.recs)) * self.topn\r\n","        freq = {}\r\n","        for u, rec in six.iteritems(self.recs):\r\n","            for r in rec:\r\n","                freq[r] = freq.get(r, 0) + 1\r\n","        ent -= sum([v / sz * math.log(v / sz) for v in six.itervalues(freq)])\r\n","        return ent\r\n","    \r\n","    def _evaluate(self):\r\n","        print('MAP@%s: %s' % (self.topn, self._map()))\r\n","        print('NDCG@%s: %s' % (self.topn, self._ndcg()))\r\n","        print('EntDiv@%s: %s' % (self.topn, self._entropy_diversity()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ln_LX0ator7M"},"source":["evaluate_func = evaluate(recs = total_rec_list, gt=gt, topn=200)\r\n","evaluate_func._evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uSQUPO8-ozJI"},"source":["## Word2vec을 이용한 추천시스템\r\n","- Tag간의 유사도\r\n","- 제목간의 유사도\r\n","- 책의 읽은 순서를 통한 유사도"]},{"cell_type":"code","metadata":{"id":"XIebCEy4oxx5"},"source":["agg = train.groupby(['user_id'])['book_id'].agg({'unique'})\r\n","agg.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJjsstnGpOaL"},"source":["# int 형식은 Word2Vec에서 학습이 안되므롤 string 형식으로 변환\r\n","sentence = []\r\n","for user_sentence in agg['unique'].values:\r\n","    sentence.append(list(map(str, user_sentence)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQewNGFCpZ6R"},"source":["# Word2Vec의 학습을 진행\r\n","from gensim.models import Word2Vec\r\n","embedding_model = Word2Vec(sentence, size=20, window=5, min_count=1, workers=4, iter=200, sg=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_L3iIm1ypjOw"},"source":["embedding_model.wv.most_similar(positive=['4893'], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xzQeeIZhpoCX"},"source":["## 전체 영화에 대해서 진행 \r\n","total_rec_list = {}\r\n","\r\n","read_list = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\r\n","for user in tqdm(train['user_id'].unique()):\r\n","    rec_list = []\r\n","    seen = read_list1[read_list1['user_id'] == user]['unique'].values[0]\r\n","    word2vec_dict = {}\r\n","    for book in seen:\r\n","        for i in embedding_model.wv.most_similar(positive=[book], topn=300):\r\n","            if i[0] not in seen:\r\n","                if i[0] not in word2vec_dict.keys():\r\n","                    word2vec_dict[i[0]] = i[1]\r\n","                else:\r\n","                    word2vec_dict[i[0]] += i[1]\r\n","    rec_list = list(dict(sorted(word2vec_dict.items(), key=lambda x:x[1], reverse=True)).keys())\r\n","    total_rec_list[user] = rec_list[0:200]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsqdNE3MqwWF"},"source":["evaluate_func = evaluate(recs = total_rec_list, gt=gt, topn=200)\r\n","evaluate_func._evaluate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kKCcvai8rQUK"},"source":["**태그를 통한 유사도 계산**"]},{"cell_type":"code","metadata":{"id":"d77mD5RDrTJV"},"source":["book_tags.columns = ['book_id', 'tag_id', 'count']\r\n","book_tags['book_id'] = book_tags['book_id'].astype(str)\r\n","book_tags['tag_id'] =  book_tags['tag_id'].astype(str)\r\n","\r\n","tags['tags_id'] = tags['tag_id'].astpye(str)\r\n","book_tags = pd.merge(book_tags, tags, how=left, on='tag_id')\r\n","book_tags.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzjwyR73rkye"},"source":["agg = book_tags.groupby(['book_id'])['tag_name'].agg({'unique'}).reset_index()\r\n","agg.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKHYXEFtsFH1"},"source":["# 태그간의 유사도 계산\r\n","#int 형식은 Word2Vec 에서 학습이 안되어서 string 형식으로 변환\r\n","sentence = []\r\n","for user_sentence in agg['unique'].values:\r\n","    sentence.append(list(map(str, user_sentence)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_4VEzZHsQwC"},"source":["for gensim.models import doc2vec\r\n","doc_vectorizer = doc2vec.Doc2Vec(\r\n","    dm=0,           # PV-DBOW / default 1\r\n","    dbow_words=1,   # w2v simultaneous with DBOW d2v / default 0\r\n","    window=10,      # distance between the predicted word and context words\r\n","    size=100,       # vector size\r\n","    alpha=0.025,    # learning_rate\r\n","    seed=1234,\r\n","    min_count=5,    #ignore with freq lower\r\n","    min_alpha=0.025 #min learning-rate\r\n","    workers=4,      #multi cpu\r\n","    hs=1,           #hierarchical softmax / default 0\r\n","    megative=10     #negative sampling / default5\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMi2D4cOtY-Z"},"source":["from collections import namedtuple\r\n","\r\n","TaggedDocument = namedtuple('TaggedDocument', 'word tags')\r\n","tagged_train_docs = [TaggedDocument(c, [d]), for c, d in agg[['unique', 'book_id']].values]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sa5Ud1iTt5cO"},"source":["doc_vectorizer.build_vocab(tagged_train_docs)\r\n","print(str(doc_vectorizer))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VH05RLl8t9tU"},"source":["# 벡터 문서 학습\r\n","from time import time\r\n","\r\n","start = time()\r\n","for epoch in tqdm(range(5)):\r\n","    doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\r\n","    doc_vectorizer.alpha -= 0.002   # decrease the learning rate\r\n","    doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay\r\n","\r\n","#doc_vectorizer.train(tagged_train_docs, total_examples=doc_vectorizer.corpus_count, epochs=doc_vectorizer.iter)\r\n","end = time()\r\n","print(\"During Time: {}\".format(end-start))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l92BrsPrvoH1"},"source":["doc_vectorizer.docvecs.most_similar('1', topn=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVe6tmrrvrk3"},"source":["train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"feG86wT9vtzH"},"source":["# tag 정보가 있는 책이 있고 아닌 책이 있어서 해당 책만 추출\r\n","agg['type'] = '1'\r\n","train = pd.merge(train, agg, how='left', on='book_id')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0KHBJX5weBg"},"source":["## 전체 영화에 대해서 진행\r\n","total_rec_list = {}\r\n","\r\n","read_list1 = train.groupby(['user_id'])['book_id'].agg({'unique'}).reset_index()\r\n","read_list2 = train[train['type']=='1'].groupby(['user_id'])['book_id'].agg({'unique'}).reset_index\r\n","\r\n","for user in tqdm(train['user_id'].unique()):\r\n","    rec_list = []\r\n","    if user in read_list2['user_id'].unique():\r\n","        seen = read_list2[read_list2['user_id']==user]['unique'].values[0]\r\n","        doc2vec_dict = {}\r\n","        for book in seen:\r\n","            for i in doc_vectorizer.docvecs.most_similar(positive=[book], topn=300):\r\n","                if i[0] not in doc2vec_dict.keys():\r\n","                    doc2vec_dict[i[0]] = i[1]\r\n","                else:\r\n","                    doc2vec_dict[i[0]] += i[1]\r\n","        rec_list = list(dict(sorted(doc2vec_dict.items(), key=lambda x:x[1], reverse=True)).keys())\r\n","    else:\r\n","        seen = read_list1[read_list1['user_id']==user]['unique'].values[0]\r\n","        for rec in popular_rec_model[0:300]:\r\n","            if rec not in seen:\r\n","                rec_list.append(rec)\r\n","    total_rec_list[user] = rec_list[0:200]\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgVcNuRbyZRJ"},"source":["evaluate_func = evaluate(recs=total_rec_list, gt=gt, topn=200)\r\n","evaluate_func._evaluate()"],"execution_count":null,"outputs":[]}]}